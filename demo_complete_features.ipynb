{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# üöÄ ChatRoutes Complete Feature Demo\n\n## Comprehensive demonstration of ChatRoutes API features\n\nThis notebook demonstrates:\n- ‚úÖ **Authentication & Setup**\n- ‚úÖ **Conversation Management**\n- ‚úÖ **Branching & Alternative Responses**\n- ‚úÖ **AutoBranch** (AI-Powered Branch Suggestions) üÜï\n- ‚úÖ **Checkpoint System** (60-70% Token Savings!)\n- ‚úÖ **Tree Visualization** (DAG Structure)\n- ‚úÖ **Message Immutability** (Cryptographic Integrity)\n- ‚úÖ **Token Optimization & Cost Savings**\n- ‚úÖ **Performance Comparison**\n\n---\n\n## ‚ö†Ô∏è Token Usage - READ THIS FIRST!\n\n**This demo is OPTIMIZED for your FREE quota (100,000 tokens/month)**\n\n### Expected Token Usage:\n- **Part 1-2** (Basic + Branching): ~2,000 tokens\n- **Part 2.5** (AutoBranch): ~500 tokens (only if service available) üÜï\n- **Part 3** (Build conversation): **CONFIGURABLE**\n  - SMALL: ~3,000 tokens (Too few for good checkpoint demo)\n  - MEDIUM: ~7,000 tokens (‚úÖ **RECOMMENDED** - Good balance!)\n  - LARGE: ~15,000 tokens (Better demo, uses more quota)\n- **Total demo**: ~9,500 tokens (MEDIUM mode = 9.5% of quota)\n\nüí° **Best Practice**: Use MEDIUM mode (default) for best checkpoint demonstration while staying quota-friendly!\n\n---\n\n**What is ChatRoutes?**\n\nChatRoutes is an advanced conversation management platform with:\n- Multi-model AI support (GPT-5, Claude Sonnet 4.5, GPT-4, etc.)\n- Conversation branching for exploring alternatives\n- **AI-powered branch detection** (AutoBranch) üÜï\n- Intelligent checkpointing for cost optimization\n- Tree/DAG visualization for understanding conversation flow\n- Enterprise-grade data immutability and security\n\n---\n\n**üìä Key Benefits Demonstrated:**\n- **60-70% token reduction** for long conversations (50-100+ messages)\n- **$17K+ annual savings** (for 10K conversations/month)\n- **2-3x faster responses** for long conversations\n- **100% immutable** message history with cryptographic hashing\n- **Complete audit trails** for HIPAA, GDPR, SOC2 compliance\n- **Automated branch detection** for smarter conversation routing üÜï",
   "metadata": {
    "id": "intro"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üì¶ Installation & Setup"
   ],
   "metadata": {
    "id": "setup"
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install --upgrade chatroutes -q\n!pip show chatroutes\nprint(\"‚úÖ ChatRoutes SDK installed successfully!\")",
   "metadata": {
    "id": "install"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "api_key = getpass('Enter your ChatRoutes API Key: ')\n",
    "os.environ['CHATROUTES_API_KEY'] = api_key\n",
    "\n",
    "print(\"‚úÖ API key configured!\")"
   ],
   "metadata": {
    "id": "config"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from chatroutes import ChatRoutes\n",
    "\n",
    "client = ChatRoutes(api_key=api_key)\n",
    "\n",
    "print(\"‚úÖ ChatRoutes client initialized!\")\n",
    "print(f\"   Base URL: {client.base_url}\")"
   ],
   "metadata": {
    "id": "client-init"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üí¨ Part 1: Basic Conversation Management"
   ],
   "metadata": {
    "id": "part1"
   }
  },
  {
   "cell_type": "code",
   "source": "print(\"Creating a fresh conversation...\\n\")\n\n# Using Claude Sonnet 4.5 for reliable demo experience\nconversation = client.conversations.create({\n    'title': f'ChatRoutes Demo {int(time.time())}',\n    'model': 'claude-sonnet-4-5'\n})\n\nprint(f\"‚úÖ Conversation created!\")\nprint(f\"   ID: {conversation['id']}\")\nprint(f\"   Title: {conversation['title']}\")\nprint(f\"   Model: claude-sonnet-4-5 (Claude Sonnet 4.5)\")\nprint(f\"   Created: {conversation['createdAt']}\")\n\nconv_id = conversation['id']",
   "metadata": {
    "id": "create-conversation"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Sending first message...\\n\")\n\nresponse = client.messages.send(\n    conv_id,\n    {\n        'content': 'Explain quantum computing in simple terms.',\n        'model': 'claude-sonnet-4-5'\n    }\n)\n\nassistant_msg = response.get('message') or response.get('assistantMessage')\n\nprint(f\"‚úÖ Message sent and response received!\\n\")\nprint(f\"AI Response ({response['model']}):\")\nprint(f\"{assistant_msg['content'][:300]}...\\n\")\n\nprint(f\"üìä Metadata:\")\nprint(f\"   Message ID: {assistant_msg['id']}\")\nprint(f\"   Tokens Used: {response.get('usage', {}).get('totalTokens', 'N/A')}\")",
   "metadata": {
    "id": "send-message"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üå≥ Part 2: Conversation Branching & Alternative Responses\n\n### What is Branching?\n\nBranching lets you explore **multiple alternative responses** from the same point in a conversation:\n- Try different **creativity levels** (temperature settings)\n- Explore different **explanation styles** (technical, analogy, ELI5)\n- Compare **multiple models** side-by-side\n- Keep **all variations** without losing the original\n\n### üéõÔ∏è Temperature Control\n\nTemperature controls AI creativity and randomness:\n\n**Claude Models (0.0-1.0):**\n- **0.0-0.3**: Conservative (factual, deterministic, consistent)\n- **0.4-0.7**: Balanced (good mix of accuracy and creativity)\n- **0.8-1.0**: Creative (diverse, imaginative, varied)\n\n**GPT Models (0.0-2.0):**\n- **0.0-0.4**: Conservative (factual, deterministic)\n- **0.5-1.0**: Balanced (standard creativity)\n- **1.1-1.5**: Creative (high creativity)\n- **1.6-2.0**: Highly creative (experimental, unpredictable)\n\n**Important:** Different AI providers have different temperature ranges. Always use values within the supported range for your chosen model.\n\n### üéØ Real-World Use Cases\n\n- **Customer Support**: Try formal vs. casual tone\n- **Content Writing**: Compare different writing styles\n- **Code Generation**: Explore multiple implementation approaches\n- **Education**: Present concepts in different difficulty levels",
   "metadata": {
    "id": "part2"
   }
  },
  {
   "cell_type": "code",
   "source": "print(\"Creating multiple branches with different creativity levels...\\n\")\n\n# Store all variations for comparison\nvariations = []\n\n# Define branch configurations\n# Each will explore \"Explain quantum computing\" with different approaches\n# NOTE: Claude models support temperature 0-1 (unlike GPT which supports 0-2)\nbranch_configs = [\n    {\n        'title': 'Conservative (Factual)',\n        'temperature': 0.2,\n        'instruction': 'Explain quantum computing in precise, technical terms. Be factual and concise.',\n        'label': 'üéØ Conservative'\n    },\n    {\n        'title': 'Balanced (Standard)',\n        'temperature': 0.7,\n        'instruction': 'Explain quantum computing in a clear, accessible way. Balance accuracy with readability.',\n        'label': '‚öñÔ∏è Balanced'\n    },\n    {\n        'title': 'Creative (Analogy)',\n        'temperature': 1.0,\n        'instruction': 'Explain quantum computing using creative analogies and metaphors. Make it fun and memorable!',\n        'label': 'üé® Creative'\n    }\n]\n\nprint(f\"Creating {len(branch_configs)} alternative responses with different styles...\\n\")\nprint(f\"üí° Note: Claude models support temperature 0-1 (for GPT models: 0-2)\\n\")\n\nfor i, config in enumerate(branch_configs, 1):\n    print(f\"[{i}/{len(branch_configs)}] Creating branch: {config['title']}\")\n    print(f\"   Temperature: {config['temperature']} | {config['label']}\")\n    \n    # Create branch\n    branch = client.branches.create(\n        conv_id,\n        {\n            'title': config['title'],\n            'contextMode': 'FULL'\n        }\n    )\n    \n    # Send message with specific temperature and instruction\n    response = client.branches.send_message(\n        conv_id,\n        branch['id'],\n        {\n            'content': config['instruction'],\n            'model': 'claude-sonnet-4-5',\n            'temperature': config['temperature']\n        }\n    )\n    \n    # Store for comparison\n    variations.append({\n        'config': config,\n        'branch_id': branch['id'],\n        'response': response.get('assistantMessage')['content']\n    })\n    \n    print(f\"   ‚úì Response received ({len(response.get('assistantMessage')['content'])} chars)\")\n    print()\n\nprint(\"‚ïê\" * 70)\nprint(\"‚úÖ All variations created successfully!\")\nprint(\"‚ïê\" * 70)\nprint(f\"   Branches created: {len(variations)}\")\nprint(f\"   Each explores the same topic with different creativity levels\")\nprint(\"‚ïê\" * 70)",
   "metadata": {
    "id": "create-branch"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"\\nüìä COMPARING ALL VARIATIONS\\n\")\nprint(\"Let's see how temperature and instructions create different responses:\\n\")\nprint(\"‚ïê\" * 70)\n\nfor i, var in enumerate(variations, 1):\n    config = var['config']\n    response = var['response']\n    \n    print(f\"\\n{config['label']} {config['title']}\")\n    print(f\"Temperature: {config['temperature']} | Instruction: {config['instruction'][:50]}...\")\n    print(\"‚îÄ\" * 70)\n    \n    # Show first 300 characters\n    preview = response[:300] + \"...\" if len(response) > 300 else response\n    print(preview)\n    print(\"‚îÄ\" * 70)\n\nprint(\"\\nüí° OBSERVATIONS:\\n\")\nprint(\"üéØ Conservative (temp 0.2):\")\nprint(\"   ‚Ä¢ More deterministic and factual\")\nprint(\"   ‚Ä¢ Technical terminology\")\nprint(\"   ‚Ä¢ Consistent structure\\n\")\n\nprint(\"‚öñÔ∏è Balanced (temp 0.7):\")\nprint(\"   ‚Ä¢ Good mix of accuracy and accessibility\")\nprint(\"   ‚Ä¢ Clear explanations\")\nprint(\"   ‚Ä¢ Natural language\\n\")\n\nprint(\"üé® Creative (temp 1.0):\")\nprint(\"   ‚Ä¢ Uses analogies and metaphors\")\nprint(\"   ‚Ä¢ More varied vocabulary\")\nprint(\"   ‚Ä¢ Engaging storytelling\\n\")\n\nprint(\"‚úÖ All variations preserved! You can:\")\nprint(\"   ‚Ä¢ Review and compare different approaches\")\nprint(\"   ‚Ä¢ Choose the best one for your use case\")\nprint(\"   ‚Ä¢ Continue conversation from any branch\")\nprint(\"   ‚Ä¢ No data lost - complete history maintained\")\n\nprint(\"\\nüí° Pro Tip: Use branching to:\")\nprint(\"   ‚Ä¢ Test different tones (formal vs casual)\")\nprint(\"   ‚Ä¢ Compare models (GPT vs Claude)\")\nprint(\"   ‚Ä¢ Explore alternative solutions\")\nprint(\"   ‚Ä¢ A/B test content variations\")",
   "metadata": {
    "id": "alt-response"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ñ Part 2.5: NEW FEATURE - AutoBranch (AI-Powered Branch Suggestions)\n\n### What is AutoBranch?\n\nAutoBranch is an intelligent system that automatically detects when conversations might benefit from branching:\n- **Automatically identifies branch points** in conversation text\n- **AI-powered analysis** to suggest where alternatives would be useful\n- **Pattern detection** for common branching scenarios\n- **Hybrid mode** combines patterns + LLM for maximum accuracy\n\n### üéØ Real-World Use Cases\n\n- **Customer Support**: Auto-detect when customer needs escalation vs self-service\n- **Sales**: Identify when prospect needs pricing vs technical information\n- **Content Creation**: Spot opportunities for different writing styles\n- **Education**: Recognize when students need simpler vs advanced explanations\n\n### üîç Detection Methods\n\n1. **Pattern-based** (Fast, rule-based): Detects keywords and common patterns\n2. **Hybrid** (Balanced): Combines patterns with LLM intelligence\n3. **LLM-only** (Most accurate): Deep AI analysis of conversation context",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"ü§ñ Testing AutoBranch - Health Check\\n\")\nprint(\"‚ïê\" * 70)\n\ntry:\n    health = client.autobranch.health()\n    print(f\"‚úÖ AutoBranch Service Status: {health['status']}\")\n    print(f\"   Service: {health['service']}\")\n    print(f\"   Version: {health['version']}\")\n    print()\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  AutoBranch health check error: {str(e)}\")\n    print(\"   This feature requires AutoBranch service to be running.\")\n    print(\"   Skipping AutoBranch demonstrations...\\n\")\n    autobranch_available = False\nelse:\n    autobranch_available = True\n\nprint(\"‚ïê\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "if autobranch_available:\n    print(\"\\nüîç Testing AutoBranch - Pattern Detection\\n\")\n    print(\"‚ïê\" * 70)\n    print(\"Scenario: Customer support conversation with multiple intent signals\")\n    print(\"‚ïê\" * 70)\n    print()\n    \n    # Example customer support text with multiple branch points\n    test_text = \"\"\"\n    Hi, I'm having trouble with my account. I can't log in and I'm not sure \n    if I should reset my password or contact technical support. Also, I wanted \n    to ask about your pricing plans for the enterprise tier. Do you offer \n    discounts for annual subscriptions?\n    \"\"\"\n    \n    print(f\"üìù Test Text:\")\n    print(test_text.strip())\n    print()\n    \n    try:\n        # Use pattern-based detection (fast, no LLM needed)\n        suggestions = client.autobranch.suggest_branches(\n            text=test_text,\n            suggestions_count=5,\n            hybrid_detection=False,  # Pure pattern detection\n            threshold=0.6  # Lower threshold to catch more potential branches\n        )\n        \n        print(f\"‚úÖ AutoBranch Analysis Complete!\\n\")\n        print(f\"üìä Results:\")\n        print(f\"   Detection Method: {suggestions['metadata']['detectionMethod']}\")\n        print(f\"   Total Branch Points Found: {suggestions['metadata']['totalBranchPointsFound']}\")\n        print(f\"   Model Used: {suggestions['metadata'].get('modelUsed', 'None (pattern-based)')}\\n\")\n        \n        if suggestions['suggestions']:\n            print(\"üåø Suggested Branches:\\n\")\n            print(\"‚îÄ\" * 70)\n            \n            for i, suggestion in enumerate(suggestions['suggestions'], 1):\n                print(f\"\\n{i}. {suggestion['title']}\")\n                print(f\"   Description: {suggestion['description']}\")\n                print(f\"   Trigger Text: '{suggestion['triggerText']}'\")\n                print(f\"   Confidence: {suggestion['confidence']:.0%}\")\n                print(f\"   Divergence Level: {suggestion['estimatedDivergence']}\")\n                print(f\"   Position: chars {suggestion['branchPoint']['start']}-{suggestion['branchPoint']['end']}\")\n                print(f\"   Reasoning: {suggestion['reasoning']}\")\n            \n            print(\"\\n\" + \"‚îÄ\" * 70)\n            print(\"\\nüí° Interpretation:\")\n            print(\"   AutoBranch detected multiple conversation paths:\")\n            print(\"   ‚Ä¢ Technical support need (login issues)\")\n            print(\"   ‚Ä¢ Sales inquiry (pricing/enterprise)\")\n            print(\"   ‚Ä¢ Decision point (password reset vs contact support)\")\n            print(\"\\n   Each could benefit from a specialized branch!\")\n        else:\n            print(\"‚ÑπÔ∏è  No clear branch points detected in this text.\")\n            print(\"   Try text with more distinct topics or decision points.\")\n        \n        print()\n    except Exception as e:\n        print(f\"‚ùå Error analyzing text: {str(e)}\")\n        print(f\"   AutoBranch service may not be available.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "if autobranch_available:\n    print(\"\\nüß† Testing AutoBranch - Hybrid Detection (Pattern + LLM)\\n\")\n    print(\"‚ïê\" * 70)\n    print(\"Using AI to enhance pattern detection for better accuracy\")\n    print(\"‚ïê\" * 70)\n    print()\n    \n    complex_text = \"\"\"\n    I've been thinking about upgrading our team's subscription. We currently \n    have 5 users on the basic plan, but we're growing fast. I'd like to \n    understand the differences between your professional and enterprise tiers, \n    especially regarding API rate limits and custom integrations. Also, one \n    of our team members reported a bug in the export feature - should I file \n    that separately?\n    \"\"\"\n    \n    print(f\"üìù Test Text (More Complex):\")\n    print(complex_text.strip())\n    print()\n    \n    try:\n        # Use hybrid detection (patterns + LLM)\n        hybrid_suggestions = client.autobranch.suggest_branches(\n            text=complex_text,\n            suggestions_count=4,\n            hybrid_detection=True,  # Enable LLM enhancement\n            threshold=0.7,\n            llm_model='gpt-4'  # Specify LLM model\n        )\n        \n        print(f\"‚úÖ Hybrid Analysis Complete!\\n\")\n        print(f\"üìä Results:\")\n        print(f\"   Detection Method: {hybrid_suggestions['metadata']['detectionMethod']}\")\n        print(f\"   Total Branch Points: {hybrid_suggestions['metadata']['totalBranchPointsFound']}\")\n        print(f\"   LLM Model: {hybrid_suggestions['metadata'].get('modelUsed', 'N/A')}\\n\")\n        \n        if hybrid_suggestions['suggestions']:\n            print(\"üåø AI-Enhanced Branch Suggestions:\\n\")\n            print(\"‚îÄ\" * 70)\n            \n            for i, suggestion in enumerate(hybrid_suggestions['suggestions'], 1):\n                print(f\"\\n{i}. {suggestion['title']}\")\n                print(f\"   {suggestion['description']}\")\n                print(f\"   Confidence: {suggestion['confidence']:.0%} | Divergence: {suggestion['estimatedDivergence']}\")\n                print(f\"   AI Reasoning: {suggestion['reasoning']}\")\n            \n            print(\"\\n\" + \"‚îÄ\" * 70)\n            print(\"\\nüí° Hybrid Detection Advantages:\")\n            print(\"   ‚úì Higher accuracy than pattern-only detection\")\n            print(\"   ‚úì Understands context and nuance\")\n            print(\"   ‚úì Better at detecting implicit branch points\")\n            print(\"   ‚úì More detailed reasoning for suggestions\")\n        \n        print()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Hybrid detection error: {str(e)}\")\n        print(f\"   Note: Hybrid mode requires LLM API configuration.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "if autobranch_available:\n    print(\"\\nüîß Practical Example: Using AutoBranch to Create Branches\\n\")\n    print(\"‚ïê\" * 70)\n    print(\"Demonstrating how to act on AutoBranch suggestions\")\n    print(\"‚ïê\" * 70)\n    print()\n    \n    simple_test = \"I need help with billing and also have a technical question about the API.\"\n    \n    print(f\"üìù User Message: '{simple_test}'\\n\")\n    \n    try:\n        # Analyze the message\n        analysis = client.autobranch.analyze_text(\n            text=simple_test,\n            suggestions_count=3,\n            threshold=0.5\n        )\n        \n        if analysis['suggestions']:\n            print(f\"‚úÖ Found {len(analysis['suggestions'])} potential branch points\\n\")\n            \n            # Show what we could do with these suggestions\n            print(\"üí° Recommended Actions:\\n\")\n            \n            for i, suggestion in enumerate(analysis['suggestions'][:2], 1):\n                print(f\"{i}. Create '{suggestion['title']}' branch\")\n                print(f\"   ‚Üí Route to: {suggestion['description']}\")\n                print(f\"   ‚Üí Confidence: {suggestion['confidence']:.0%}\")\n                print(f\"   ‚Üí Would handle: '{suggestion['triggerText']}'\")\n                print()\n            \n            print(\"‚îÄ\" * 70)\n            print(\"\\nüéØ Integration Pattern (Pseudocode):\")\n            print(\"\"\"\n            # In your application:\n            1. User sends message\n            2. Call autobranch.analyze_text(message)\n            3. If high-confidence suggestions found:\n               - Auto-create branches for top suggestions\n               - Route conversation to appropriate branch\n               - Assign to right team/workflow\n            4. Continue conversation in specialized branch\n            \"\"\")\n            print(\"‚îÄ\" * 70)\n            \n        else:\n            print(\"‚ÑπÔ∏è  No clear branch points - continue in main conversation\")\n        \n        print()\n    except Exception as e:\n        print(f\"‚ùå Error: {str(e)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### üìä AutoBranch Summary\n\n**Key Benefits:**\n- **Automated Detection**: No manual analysis needed\n- **Smart Routing**: Route conversations to appropriate handlers\n- **Improved UX**: Users get specialized attention faster\n- **Scalability**: Handle complex conversations with multiple intents\n\n**When to Use:**\n- Customer support triage (technical vs billing vs sales)\n- Multi-intent conversations (user asks multiple questions)\n- Routing decisions (which team/bot should handle this?)\n- Quality assurance (ensure all customer needs are addressed)\n\n**Configuration Options:**\n- `suggestions_count`: How many suggestions to return (1-10)\n- `hybrid_detection`: Use LLM for enhanced accuracy\n- `threshold`: Minimum confidence level (0.0-1.0)\n- `llm_model`: Which LLM to use for hybrid mode",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## üéØ Part 3: Building a Long Conversation (Setup for Checkpoints)\n\n### ‚ö†Ô∏è Token Usage Notice\n\nThis section creates a conversation to demonstrate checkpoints.\n\n**Options:**\n- **SMALL** (3 messages): ~3K tokens - Too few for meaningful checkpoint\n- **MEDIUM** (5 messages): ~6K tokens - ‚úÖ **RECOMMENDED** (Good balance!)\n- **LARGE** (10 messages): ~15K tokens - Better demo but uses more quota\n\n**Your FREE quota: 100,000 tokens/month**\n\nüí° **Note:** Checkpoints show MAXIMUM value with 50-100+ messages. This demo proves the concept works, not maximum savings!",
   "metadata": {
    "id": "part3"
   }
  },
  {
   "cell_type": "code",
   "source": "# ‚öôÔ∏è CONFIGURATION: Choose your demo size\n# Change this to 'SMALL', 'MEDIUM', or 'LARGE'\nDEMO_SIZE = 'MEDIUM'  # üëà RECOMMENDED - Best balance for checkpoint demo!\n\n# Topic sets for different demo sizes\nTOPICS = {\n    'SMALL': [\n        \"What is Python?\",\n        \"Explain lists vs tuples\",\n        \"What are decorators?\"\n    ],\n    'MEDIUM': [\n        \"What is Python?\",\n        \"Explain lists vs tuples\", \n        \"What are decorators?\",\n        \"Describe generators\",\n        \"What is asyncio?\",\n        \"Explain context managers\",\n        \"What are metaclasses?\"\n    ],\n    'LARGE': [\n        \"What is machine learning?\",\n        \"Explain supervised learning\",\n        \"What are neural networks?\",\n        \"Describe CNNs briefly\",\n        \"What is transfer learning?\",\n        \"Explain gradient descent\",\n        \"What is backpropagation?\",\n        \"Describe transformers\",\n        \"What is BERT?\",\n        \"Explain GPT architecture\"\n    ]\n}\n\ntopics = TOPICS[DEMO_SIZE]\nestimated_tokens = len(topics) * 1000  # More accurate estimate with \"(Keep response under 100 words)\"\n\nprint(\"‚ïê\" * 70)\nprint(f\"üìä DEMO CONFIGURATION: {DEMO_SIZE}\")\nprint(\"‚ïê\" * 70)\nprint(f\"   Messages to create: {len(topics)} exchanges ({len(topics) * 2} total messages)\")\nprint(f\"   Estimated tokens: ~{estimated_tokens:,}\")\nprint(f\"   Your FREE quota: 100,000 tokens/month\")\nprint(f\"   Percentage of quota: ~{(estimated_tokens/100000)*100:.1f}%\")\nprint(\"‚ïê\" * 70)\nprint()\n\n# Checkpoint readiness check\nif len(topics) < 5:\n    print(\"‚ö†Ô∏è  NOTE: This conversation is too short for a meaningful checkpoint demo.\")\n    print(\"   Checkpoints show REAL value with 50-100+ messages.\")\n    print(\"   This will demonstrate HOW it works, not maximum savings.\\n\")\nelif len(topics) >= 5 and len(topics) < 10:\n    print(\"‚úÖ GOOD: This size is perfect for demonstrating checkpoint technology.\")\n    print(\"   Remember: Real production value appears with 50-100+ messages.\\n\")\n\n# Safety check for LARGE demos\nif DEMO_SIZE == 'LARGE':\n    print(\"‚ö†Ô∏è  WARNING: LARGE demo will use ~15% of your monthly quota!\")\n    proceed = input(\"   Type 'yes' to proceed: \")\n    if proceed.lower() != 'yes':\n        print(\"   Demo cancelled. Try DEMO_SIZE = 'MEDIUM' instead.\")\n        raise SystemExit(\"Demo cancelled by user\")\n    print()\n\nprint(\"Creating a conversation to demonstrate checkpoints...\\n\")\n\n# Create conversation\nlong_conv = client.conversations.create({\n    'title': f'Demo {DEMO_SIZE} ({int(time.time())})',\n    'model': 'claude-sonnet-4-5'\n})\n\nlong_conv_id = long_conv['id']\nprint(f\"‚úÖ Conversation created: {long_conv_id}\\n\")\n\nprint(f\"Sending {len(topics)} messages (with concise responses)...\\n\")\n\nmessage_count = 0\ntotal_tokens_used = 0\nresponses = []\n\nfor i, topic in enumerate(topics, 1):\n    print(f\"[{i}/{len(topics)}] {topic}\")\n    \n    try:\n        # Add instruction to keep response brief to save tokens\n        content = f\"{topic} (Keep response under 100 words)\"\n        \n        resp = client.messages.send(\n            long_conv_id,\n            {\n                'content': content,\n                'model': 'claude-sonnet-4-5'\n            }\n        )\n        \n        message_count += 2  # user + assistant\n        tokens = resp.get('usage', {}).get('totalTokens', 0)\n        total_tokens_used += tokens\n        responses.append(resp)\n        \n        print(f\"   ‚úì Response received ({tokens:,} tokens)\")\n        \n        time.sleep(0.5)  # Rate limiting\n    except Exception as e:\n        error_msg = str(e)\n        if 'Quota exceeded' in error_msg:\n            print(f\"   ‚úó Quota exceeded! You've used your monthly limit.\")\n            print(f\"   ‚ÑπÔ∏è  Consider upgrading to PRO (5M tokens/month)\")\n            break\n        else:\n            print(f\"   ‚úó Error: {error_msg}\")\n            print(f\"   Continuing with next message...\")\n        continue\n\nprint(f\"\\n{'‚ïê' * 70}\")\nprint(f\"‚úÖ CONVERSATION CREATED\")\nprint(f\"{'‚ïê' * 70}\")\nprint(f\"   Messages created: {message_count}\")\nprint(f\"   Actual tokens used: {total_tokens_used:,}\")\nprint(f\"   Remaining quota: ~{100000 - total_tokens_used:,} tokens\")\nprint(f\"{'‚ïê' * 70}\\n\")\n\nif total_tokens_used < 1000:\n    print(\"‚ö†Ô∏è  Note: Very few tokens used. Check if API calls succeeded.\")\nelif DEMO_SIZE == 'SMALL' and total_tokens_used < 5000:\n    print(\"‚úÖ Great! You used minimal tokens and can run this demo many times!\")\n    print(\"   Feel free to try DEMO_SIZE = 'MEDIUM' next.\")\nelif DEMO_SIZE == 'MEDIUM' and total_tokens_used < 10000:\n    print(\"‚úÖ Good! You have plenty of quota left to explore more features.\")\n    print(f\"   You can run this demo ~{int((100000-total_tokens_used)/total_tokens_used)} more times!\")\nelse:\n    print(\"‚ÑπÔ∏è  You used a significant portion of your quota.\")\n    print(\"   Consider the smaller DEMO_SIZE options for future runs.\")",
   "metadata": {
    "id": "long-conversation"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# üìä Visualize your quota usage so far\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Calculate usage (estimate Parts 1-2)\nparts_1_2_estimate = 2000\ncumulative_used = parts_1_2_estimate + total_tokens_used\nquota = 100000\nremaining = quota - cumulative_used\npercent_used = (cumulative_used / quota) * 100\n\nfig, ax = plt.subplots(figsize=(12, 3))\n\n# Determine status and color\nif cumulative_used < 20000:\n    bar_color = '#4CAF50'  # Green\n    status_emoji = '‚úÖ'\n    status_text = 'Excellent'\nelif cumulative_used < 50000:\n    bar_color = '#FFC107'  # Yellow\n    status_emoji = '‚ö†Ô∏è'\n    status_text = 'Moderate'\nelse:\n    bar_color = '#f44336'  # Red\n    status_emoji = '‚ùå'\n    status_text = 'High'\n\n# Draw quota bars\nax.barh(0, cumulative_used, height=0.6, color=bar_color, label=f'Used: {cumulative_used:,} tokens', edgecolor='black', linewidth=2)\nax.barh(0, remaining, left=cumulative_used, height=0.6, color='#e8e8e8', label=f'Remaining: {remaining:,} tokens', edgecolor='gray', linewidth=1)\n\n# Add zone markers\nax.axvline(20000, color='green', linestyle='--', alpha=0.4, linewidth=2, label='Safe Zone')\nax.axvline(50000, color='orange', linestyle='--', alpha=0.4, linewidth=2, label='Caution Zone')\nax.axvline(80000, color='red', linestyle='--', alpha=0.4, linewidth=2, label='Critical Zone')\n\n# Labels and formatting\nax.set_xlim(0, quota)\nax.set_ylim(-0.5, 0.5)\nax.set_xlabel('Tokens', fontsize=13, fontweight='bold')\nax.set_title(f'{status_emoji} Your FREE Quota Usage: {status_text} ({percent_used:.1f}% used)', \n             fontsize=15, fontweight='bold', pad=20)\nax.set_yticks([])\nax.legend(loc='upper right', fontsize=10, framealpha=0.9)\n\n# Add percentage text on bar\nif cumulative_used > 5000:\n    ax.text(cumulative_used / 2, 0, f'{percent_used:.1f}%', \n            ha='center', va='center', fontsize=16, fontweight='bold', \n            color='white' if bar_color != '#FFC107' else 'black',\n            bbox=dict(boxstyle='round,pad=0.3', facecolor=bar_color, alpha=0.8, edgecolor='black', linewidth=2))\n\n# Add milestone markers\nmilestones = [25000, 50000, 75000]\nfor milestone in milestones:\n    if milestone <= quota:\n        ax.text(milestone, -0.35, f'{milestone//1000}K', ha='center', va='top', fontsize=9, color='gray')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nüí° Usage Analysis:\")\nprint(f\"   Demo size used: {DEMO_SIZE}\")\nprint(f\"   Tokens consumed: {cumulative_used:,} ({percent_used:.1f}% of quota)\")\nprint(f\"   Remaining: {remaining:,} tokens\")\nif percent_used < 10:\n    print(f\"   {status_emoji} Great! You can run this demo {int(remaining / estimated_tokens)} more times!\")\nelif percent_used < 30:\n    print(f\"   {status_emoji} Good! Plenty of quota left for exploration.\")\nelse:\n    print(f\"   {status_emoji} Consider using SMALL mode for future runs to conserve quota.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üîñ Part 4: NEW FEATURE - Checkpoint System\n\n### What are Checkpoints?\n\nCheckpoints are AI-generated summaries of conversation history that:\n- **Reduce tokens by 60-70%** for long conversations (50-100+ messages)\n- **Maintain context** while optimizing cost\n- **Improve response speed** by 2-3x\n- **Auto-create** every 50 messages (configurable)\n\n### ‚ö†Ô∏è Demo Honesty: Small Conversation Example\n\n**This demo conversation has 7-14 messages - enough to:**\n- ‚úÖ Show HOW checkpoints work (AI summarization)\n- ‚úÖ Prove the technology functions correctly\n- ‚ùå NOT show maximum token savings (too few messages)\n\n**Real checkpoint value appears with 50-100+ messages:**\n- Long customer support conversations\n- Multi-session knowledge gathering\n- Extended research discussions\n\n### üìä Visual Explanation: How Checkpoints Work\n\n```\nWITHOUT Checkpoints (Traditional):\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Send ALL 150 messages to AI  ‚Üí  15,000 tokens             ‚îÇ\n‚îÇ  ‚ö†Ô∏è Slow response + High cost                               ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nWITH Checkpoints (ChatRoutes):\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  Checkpoint Summary (500 tokens)                            ‚îÇ\n‚îÇ      +                                                       ‚îÇ\n‚îÇ  Recent 50 messages (5,000 tokens)                          ‚îÇ\n‚îÇ      =                                                       ‚îÇ\n‚îÇ  Total: 5,500 tokens  ‚Üí  63% SAVINGS!                      ‚îÇ\n‚îÇ  ‚úÖ Fast response + Low cost                                ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### üéØ The Magic Formula:\nInstead of sending **ALL messages**, send:\n1. **AI Summary** of old messages (compact: ~500 tokens)\n2. **Recent messages** for context (last 50: ~5K tokens)\n\nResult: **60-70% token reduction** while maintaining full context!\n\n**Think of this demo as \"Hello World\" for checkpoints - proves it works!**",
   "metadata": {
    "id": "part4"
   }
  },
  {
   "cell_type": "code",
   "source": "print(\"Creating a checkpoint for demonstration...\\n\")\n\n# Get conversation with messages\nconversation_data = client.conversations.get(long_conv_id)\nmessages = conversation_data.get('messages', [])\n\nprint(f\"üìä Conversation has {len(messages)} messages\")\nprint(f\"üí° NOTE: This is a PROOF-OF-CONCEPT checkpoint demo.\")\nprint(f\"   Real production value appears with 50-100+ messages!\\n\")\n\nif len(messages) > 0:\n    # Find an anchor message (use the middle message)\n    anchor_message = messages[len(messages) // 2]\n    anchor_message_id = anchor_message['id']\n    \n    print(f\"Creating checkpoint at message {len(messages) // 2}...\\n\")\n    \n    # Get main branch ID from the first message's branchId\n    # (All messages start on the main branch)\n    if messages[0].get('branchId'):\n        branch_id_for_checkpoint = messages[0]['branchId']\n        \n        checkpoint = client.checkpoints.create(\n            long_conv_id,\n            branch_id=branch_id_for_checkpoint,\n            anchor_message_id=anchor_message_id\n        )\n        \n        print(f\"‚úÖ Checkpoint created successfully!\\n\")\n        print(f\"üìã Checkpoint Details:\")\n        print(f\"   ID: {checkpoint['id']}\")\n        print(f\"   Anchor Message: {checkpoint.get('anchorMessageId') or checkpoint.get('anchor_message_id')}\")\n        print(f\"   Summary Length: {checkpoint.get('tokenCount') or checkpoint.get('token_count')} tokens\")\n        print(f\"   Created: {checkpoint.get('createdAt') or checkpoint.get('created_at')}\\n\")\n        \n        print(f\"üìù AI-Generated Summary:\")\n        print(f\"{checkpoint['summary']}\\n\")\n        \n        # Calculate demo stats\n        estimated_original_tokens = len(messages) * 150\n        checkpoint_tokens = checkpoint.get('tokenCount') or checkpoint.get('token_count')\n        demo_reduction = ((estimated_original_tokens - checkpoint_tokens) / estimated_original_tokens) * 100\n        \n        print(f\"‚îÄ\" * 70)\n        print(f\"üìä DEMO STATS (Small Conversation):\")\n        print(f\"‚îÄ\" * 70)\n        print(f\"   Original messages: {len(messages)} (~{estimated_original_tokens} tokens)\")\n        print(f\"   Checkpoint summary: {checkpoint_tokens} tokens\")\n        print(f\"   Reduction: {demo_reduction:.0f}%\")\n        print(f\"\\nüéØ SCALING TO PRODUCTION:\")\n        print(f\"   With 150 messages: Would save ~9,500 tokens (63% reduction)\")\n        print(f\"   With 500 messages: Would save ~44,500 tokens (89% reduction)\")\n        print(f\"   The longer the conversation, the bigger the savings!\")\n        print(f\"‚îÄ\" * 70)\n        print()\n        \n        checkpoint_id = checkpoint['id']\n    else:\n        print(\"‚ùå Could not find branch ID in messages\")\n        print(\"   This might be an older conversation without branch support\")\nelse:\n    print(\"‚ùå No messages found in conversation\")",
   "metadata": {
    "id": "create-checkpoint"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Listing all checkpoints for this conversation...\\n\")\n",
    "\n",
    "checkpoints = client.checkpoints.list(long_conv_id)\n",
    "\n",
    "print(f\"‚úÖ Found {len(checkpoints)} checkpoint(s)\\n\")\n",
    "\n",
    "for i, cp in enumerate(checkpoints, 1):\n",
    "    token_count = cp.get('tokenCount') or cp.get('token_count')\n",
    "    created_at = cp.get('createdAt') or cp.get('created_at')\n",
    "    \n",
    "    print(f\"Checkpoint {i}:\")\n",
    "    print(f\"   ID: {cp['id'][:16]}...\")\n",
    "    print(f\"   Tokens: {token_count}\")\n",
    "    print(f\"   Created: {created_at}\")\n",
    "    print(f\"   Summary: {cp['summary'][:100]}...\")\n",
    "    print()"
   ],
   "metadata": {
    "id": "list-checkpoints"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"Demonstrating immutability features...\\n\")\n\n# Get conversation to show contentHash\nconv_data = client.conversations.get(conv_id)\nall_messages = conv_data.get('messages', [])\n\nif len(all_messages) > 0:\n    sample_message = all_messages[0]\n    content_hash = sample_message.get('contentHash')\n    \n    print(\"üìù Message with Cryptographic Hash:\")\n    print(\"‚îÄ\" * 60)\n    print(f\"   Message ID: {sample_message['id']}\")\n    print(f\"   Role: {sample_message['role']}\")\n    print(f\"   Content: {sample_message['content'][:60]}...\")\n    \n    if content_hash:\n        print(f\"   Content Hash: {content_hash[:16]}...\")\n        print(f\"   Created: {sample_message.get('createdAt', 'N/A')}\")\n        print(\"‚îÄ\" * 60)\n        print(\"\\n‚úÖ This SHA-256 hash PROVES the message hasn't been altered!\")\n        print(\"   Any modification would change the hash.\\n\")\n    else:\n        print(f\"   Content Hash: Not yet calculated\")\n        print(f\"   Created: {sample_message.get('createdAt', 'N/A')}\")\n        print(\"‚îÄ\" * 60)\n        print(\"\\nüí° NOTE: Content hash will be calculated on next update.\")\n        print(\"   New messages automatically get hashes on creation.\\n\")\n    \n    print(\"üîí Immutability in Action:\")\n    print(\"   1. Messages are WRITE-ONCE (cannot be modified)\")\n    print(\"   2. Updates create NEW versions (not edits)\")\n    print(\"   3. Deletes are SOFT (marked, not removed)\")\n    print(\"   4. Full audit trail maintained\")\n    print(\"   5. Compliance-ready (HIPAA, GDPR, SOC2)\\n\")\n    \n    print(\"üí° Why This Matters:\")\n    print(\"   ‚Ä¢ Legal/medical records: Cannot be tampered with\")\n    print(\"   ‚Ä¢ Audit trails: Complete history preserved\")\n    print(\"   ‚Ä¢ Regulatory compliance: Meets strictest requirements\")\n    print(\"   ‚Ä¢ Data integrity: Cryptographically guaranteed\")\n    \nelse:\n    print(\"‚ö†Ô∏è  No messages available for demonstration\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê Part 5: Message Immutability & Data Integrity\n",
    "\n",
    "### What is Immutability?\n",
    "\n",
    "ChatRoutes ensures **100% immutable messages** meaning:\n",
    "- **Messages cannot be modified** after creation\n",
    "- Every message has a **cryptographic hash** (SHA-256)\n",
    "- Updates create **new versions** (not modifications)\n",
    "- Deletions are **soft** (marked deleted, not removed)\n",
    "- Complete **audit trail** for compliance\n",
    "\n",
    "This is critical for:\n",
    "- ‚úÖ HIPAA compliance (healthcare)\n",
    "- ‚úÖ GDPR compliance (data protection)\n",
    "- ‚úÖ SOC2 compliance (security)\n",
    "- ‚úÖ Legal/audit trails\n",
    "- ‚úÖ Data integrity guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Concept 1: Updates Create NEW Messages (Not Modifications)\n",
    "\n",
    "#### ‚ùå Traditional Systems (Mutable):\n",
    "```sql\n",
    "UPDATE messages SET content = 'new' WHERE id = 'msg_123'\n",
    "```\n",
    "‚Üí Original data **LOST forever**\n",
    "\n",
    "#### ‚úÖ ChatRoutes (Immutable):\n",
    "1. Original message **preserved** with hash\n",
    "2. Create **NEW message** with updated content\n",
    "3. Link them with **version tracking**\n",
    "\n",
    "‚Üí Complete audit trail maintained!\n",
    "\n",
    "**Let's see this in action:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ DEMONSTRATION: Updates Create New Messages\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a test conversation\n",
    "test_conv = client.conversations.create({\n",
    "    'title': 'Immutability Demo',\n",
    "    'model': 'claude-sonnet-4-5'\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Created test conversation: {test_conv['id']}\\n\")\n",
    "\n",
    "# Send original message\n",
    "print(\"üì§ Step 1: Creating original message...\")\n",
    "original = client.messages.send(\n",
    "    test_conv['id'],\n",
    "    {'content': 'What is 2 + 2?', 'model': 'claude-sonnet-4-5'}\n",
    ")\n",
    "\n",
    "original_msg = original.get('assistantMessage') or original.get('message')\n",
    "original_id = original_msg['id']\n",
    "original_hash = original_msg.get('contentHash', 'N/A')\n",
    "\n",
    "print(f\"   ‚úÖ Original Message ID: {original_id}\")\n",
    "print(f\"   Content: {original_msg['content'][:60]}...\")\n",
    "print(f\"   Hash: {original_hash[:16] if original_hash != 'N/A' else 'N/A'}...\\n\")\n",
    "\n",
    "# Send 'correction' message\n",
    "print(\"üì§ Step 2: Creating 'corrected' message...\")\n",
    "correction = client.messages.send(\n",
    "    test_conv['id'],\n",
    "    {'content': 'Actually, let me clarify my question.', 'model': 'claude-sonnet-4-5'}\n",
    ")\n",
    "\n",
    "corrected_msg = correction.get('assistantMessage') or correction.get('message')\n",
    "corrected_id = corrected_msg['id']\n",
    "\n",
    "print(f\"   ‚úÖ New Message ID: {corrected_id}\\n\")\n",
    "\n",
    "# Show both still exist\n",
    "print(\"‚úÖ RESULT: Both messages exist independently!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   Original: {original_id} (still exists unchanged)\")\n",
    "print(f\"   New:      {corrected_id} (separate message)\")\n",
    "print(\"\\nüí° Key Point: The original message is PRESERVED forever!\")\n",
    "\n",
    "# Store conversation ID for cleanup\n",
    "demo_conv_id = test_conv['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™¶ Concept 2: Soft Deletes (Tombstone Pattern)\n",
    "\n",
    "When you \"delete\" a message in ChatRoutes:\n",
    "\n",
    "#### ‚ùå What DOESN'T happen:\n",
    "- Message row is NOT removed from database\n",
    "- Content is NOT erased\n",
    "- Hash is NOT deleted\n",
    "\n",
    "#### ‚úÖ What DOES happen:\n",
    "- `deletedAt` timestamp is set (e.g., 2025-11-06 10:30:00)\n",
    "- `deleteReason` is recorded\n",
    "- Message becomes 'tombstone' (marked but preserved)\n",
    "- Audit log entry created (who, when, why)\n",
    "\n",
    "#### üíæ Database State After Deletion:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Message Record (STILL IN DATABASE)                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ id: msg_abc123                                      ‚îÇ\n",
    "‚îÇ content: \"What is 2 + 2?\"                          ‚îÇ\n",
    "‚îÇ contentHash: a3f5e1b...                             ‚îÇ\n",
    "‚îÇ deletedAt: 2025-11-06 10:30:00 ‚Üê TOMBSTONE MARKER  ‚îÇ\n",
    "‚îÇ deleteReason: 'User requested deletion'            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**The data is still there - just marked as deleted!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Concept 3: Complete Audit Trail\n",
    "\n",
    "Every action creates an audit log entry:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Audit Log Table                                          ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ messageId    ‚îÇ action  ‚îÇ userId  ‚îÇ timestamp  ‚îÇ metadata ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ msg_abc123   ‚îÇ CREATE  ‚îÇ user_1  ‚îÇ 10:25:00   ‚îÇ {...}    ‚îÇ\n",
    "‚îÇ msg_abc123   ‚îÇ VIEW    ‚îÇ user_2  ‚îÇ 10:28:00   ‚îÇ {...}    ‚îÇ\n",
    "‚îÇ msg_abc123   ‚îÇ DELETE  ‚îÇ user_1  ‚îÇ 10:30:00   ‚îÇ {reason} ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "#### ‚úÖ Benefits:\n",
    "- Who did what, when, and why\n",
    "- Complete history for forensics\n",
    "- Regulatory compliance (HIPAA, GDPR, SOC2)\n",
    "- Data can be 'undeleted' if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîê Concept 4: Cryptographic Hash Verification\n",
    "\n",
    "Every message has a **SHA-256 hash** that proves data integrity.\n",
    "\n",
    "**Let's verify a message hash:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "print(\"üîê DEMONSTRATION: Hash Verification\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get a message with hash\n",
    "conv_data = client.conversations.get(demo_conv_id)\n",
    "messages = conv_data.get('messages', [])\n",
    "\n",
    "if len(messages) > 0:\n",
    "    message = messages[0]\n",
    "    stored_hash = message.get('contentHash')\n",
    "    \n",
    "    if stored_hash:\n",
    "        print(\"üìù Message Data:\")\n",
    "        print(f\"   ID: {message['id']}\")\n",
    "        print(f\"   Content: {message['content'][:60]}...\")\n",
    "        print(f\"   Stored Hash: {stored_hash}\\n\")\n",
    "        \n",
    "        # Recalculate hash (same algorithm as backend)\n",
    "        canonical_data = {\n",
    "            \"v\": 1,\n",
    "            \"role\": message['role'],\n",
    "            \"content\": message['content'],\n",
    "            \"model\": message.get('model'),\n",
    "            \"parentMessageId\": message.get('parentMessageId'),\n",
    "            \"branchId\": message.get('branchId'),\n",
    "            \"createdAt\": message.get('createdAt')\n",
    "        }\n",
    "        \n",
    "        canonical_json = json.dumps(canonical_data, separators=(',', ':'))\n",
    "        calculated_hash = hashlib.sha256(canonical_json.encode()).hexdigest()\n",
    "        \n",
    "        print(\"üîç Hash Verification:\")\n",
    "        print(f\"   Stored:     {stored_hash}\")\n",
    "        print(f\"   Calculated: {calculated_hash}\\n\")\n",
    "        \n",
    "        if calculated_hash == stored_hash:\n",
    "            print(\"   ‚úÖ MATCH! Message data is authentic and unchanged!\")\n",
    "        else:\n",
    "            print(\"   ‚ùå MISMATCH! Data may have been tampered with!\")\n",
    "        \n",
    "        # Show what happens with tampering\n",
    "        print(\"\\nüî¨ What Happens if Data is Tampered?\\n\")\n",
    "        \n",
    "        tampered_data = canonical_data.copy()\n",
    "        tampered_data['content'] = message['content'] + \"X\"  # Add one character\n",
    "        \n",
    "        tampered_json = json.dumps(tampered_data, separators=(',', ':'))\n",
    "        tampered_hash = hashlib.sha256(tampered_json.encode()).hexdigest()\n",
    "        \n",
    "        print(f\"   Original hash:  {calculated_hash[:32]}...\")\n",
    "        print(f\"   Tampered hash:  {tampered_hash[:32]}...\")\n",
    "        print(f\"\\n   ‚ùå COMPLETELY DIFFERENT! Tampering detected immediately.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Message doesn't have hash yet (older message)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No messages available\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° How Hash Verification Works\n",
    "\n",
    "#### ‚ùå Common Misconception:\n",
    "\"Can I decrypt the hash to get the message back?\"\n",
    "\n",
    "**NO!** SHA-256 is NOT encryption - it's a **ONE-WAY hash function**.\n",
    "\n",
    "#### ‚úÖ How It Actually Works:\n",
    "\n",
    "```\n",
    "Verification Process:\n",
    "1. Take original message data from database\n",
    "2. Recalculate hash using same algorithm  \n",
    "3. Compare: New hash === Stored hash?\n",
    "   ‚Ä¢ Match = Data unchanged ‚úÖ\n",
    "   ‚Ä¢ Mismatch = Data tampered ‚ùå\n",
    "```\n",
    "\n",
    "#### üîê Why This is Powerful:\n",
    "\n",
    "- **Cannot reverse**: Hash ‚Üí Original data (impossible)\n",
    "- **Can verify**: Original data ‚Üí Hash (easy)\n",
    "- **Tamper-proof**: Any change = Different hash\n",
    "- **Deterministic**: Same input = Same hash (always)\n",
    "\n",
    "#### üéØ Real-World Applications:\n",
    "\n",
    "- **Medical records**: Prove records haven't been altered\n",
    "- **Legal documents**: Verify authenticity in court\n",
    "- **Audit trails**: Complete tamper-proof history\n",
    "- **Compliance**: Meet HIPAA, GDPR, SOC2 requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè• Real-World Use Cases\n",
    "\n",
    "#### üìä Healthcare (HIPAA):\n",
    "- Doctor updates patient notes ‚Üí New version, old preserved\n",
    "- Complete audit trail for malpractice defense\n",
    "- Prove notes weren't altered after incident\n",
    "\n",
    "#### ‚öñÔ∏è Legal/Financial:\n",
    "- Contract negotiations ‚Üí Every revision tracked\n",
    "- Deleted emails recoverable for discovery\n",
    "- Cryptographic proof of original content\n",
    "\n",
    "#### üîí Security/Compliance:\n",
    "- Data breach investigation ‚Üí Complete history\n",
    "- Regulatory audits ‚Üí Unalterable records\n",
    "- Insider threat detection ‚Üí Who changed what\n",
    "\n",
    "### ‚úÖ Key Takeaways:\n",
    "\n",
    "1. **Messages are NEVER truly deleted or modified**\n",
    "2. **All changes create NEW records with audit trails**\n",
    "3. **Cryptographic hashes prove data integrity**\n",
    "4. **Complete history preserved for compliance**\n",
    "5. **Original data always verifiable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up demo conversation\n",
    "try:\n",
    "    client.conversations.delete(demo_conv_id)\n",
    "    print(\"üßπ Demo conversation cleaned up (soft-deleted, of course!)\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"Getting conversation tree structure...\\n\")\n\ntry:\n    # Note: This requires the SDK to support tree endpoint\n    # For now, we'll build a simple tree from branches\n    tree_data = client.conversations.get(conv_id)\n    \n    branches = tree_data.get('branches', [])\n    messages_count = len(tree_data.get('messages', []))\n    \n    print(f\"‚úÖ Conversation Tree:\")\n    print(f\"   Total branches: {len(branches)}\")\n    print(f\"   Total messages: {messages_count}\\n\")\n    \n    print(\"üìä Branch Structure:\")\n    print(\"‚îÄ\" * 60)\n    \n    for i, branch in enumerate(branches, 1):\n        is_main = branch.get('isMain', False)\n        branch_icon = \"üå≥\" if is_main else \"üå±\"\n        branch_type = \"[MAIN]\" if is_main else \"[BRANCH]\"\n        msg_count = branch.get('messageCount', 0)\n        \n        print(f\"{branch_icon} {branch_type} {branch['title']}\")\n        print(f\"   ID: {branch['id'][:20]}...\")\n        print(f\"   Messages: {msg_count}\")\n        print(f\"   Created: {branch.get('createdAt', 'N/A')}\")\n        if i < len(branches):\n            print()\n    \n    print(\"‚îÄ\" * 60)\n    print(\"\\nüí° The tree structure shows all conversation paths explored!\")\n    print(\"   Each branch represents an alternative exploration.\")\n    print(\"\\nüìù Message Count Note:\")\n    print(\"   Each conversation exchange = 2 messages (user + assistant)\")\n    print(\"   So '4 messages' means 2 exchanges (2 question-answer pairs)\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Could not fetch tree: {str(e)}\")\n    print(\"   Tree visualization requires conversation with branches.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# üìà Token Growth Comparison Chart\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\n# Data points (renamed to avoid conflict with conversation messages)\nmessage_counts = np.array([10, 25, 50, 75, 100, 150, 200, 300, 500])\nwithout_checkpoints = message_counts * 100  # Linear growth\nwith_checkpoints = np.where(message_counts <= 50, message_counts * 100, 500 + (50 * 100))  # Flattens after checkpoint\n\n# Plot lines\nline1 = ax.plot(message_counts, without_checkpoints, 'r-o', linewidth=3, markersize=10, \n                label='‚ùå Without Checkpoints (Linear Growth)', markeredgecolor='darkred', markeredgewidth=2)\nline2 = ax.plot(message_counts, with_checkpoints, 'g-s', linewidth=3, markersize=10,\n                label='‚úÖ With Checkpoints (Controlled Growth)', markeredgecolor='darkgreen', markeredgewidth=2)\n\n# Fill area between lines to show savings\nax.fill_between(message_counts, without_checkpoints, with_checkpoints, \n                where=(message_counts > 50), alpha=0.3, color='gold', label='üí∞ Token Savings')\n\n# Checkpoint trigger line\nax.axvline(x=50, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='üîñ Checkpoint Created (50 msgs)')\n\n# Add annotations\nax.annotate('Checkpoint kicks in!\\nSavings start here',\n            xy=(50, 5000), xytext=(100, 8000),\n            arrowprops=dict(arrowstyle='->', lw=2, color='orange'),\n            fontsize=11, fontweight='bold', color='darkorange',\n            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', edgecolor='orange', linewidth=2))\n\n# Highlight massive savings at 500 messages\nsavings_500 = without_checkpoints[-1] - with_checkpoints[-1]\nax.annotate(f'Save {savings_500:,} tokens!\\n({((savings_500/without_checkpoints[-1])*100):.0f}% reduction)',\n            xy=(500, with_checkpoints[-1]), xytext=(400, 35000),\n            arrowprops=dict(arrowstyle='->', lw=2, color='green'),\n            fontsize=12, fontweight='bold', color='darkgreen',\n            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', edgecolor='green', linewidth=2))\n\n# Styling\nax.set_xlabel('Number of Messages in Conversation', fontsize=14, fontweight='bold')\nax.set_ylabel('Tokens Sent to AI per Request', fontsize=14, fontweight='bold')\nax.set_title('üöÄ ChatRoutes Checkpoint System: Token Usage Over Time', \n             fontsize=16, fontweight='bold', pad=20)\nax.legend(fontsize=11, loc='upper left', framealpha=0.95, edgecolor='black', fancybox=True)\nax.grid(True, alpha=0.3, linestyle=':', linewidth=1)\nax.set_xlim(0, 550)\nax.set_ylim(0, max(without_checkpoints) * 1.1)\n\n# Add data labels at key points\nkey_messages = [50, 150, 500]\nfor msg in key_messages:\n    idx = np.where(message_counts == msg)[0][0]\n    \n    # Without checkpoints\n    ax.text(msg, without_checkpoints[idx] + 1500, f'{int(without_checkpoints[idx]):,}',\n            ha='center', va='bottom', fontsize=9, color='darkred', fontweight='bold')\n    \n    # With checkpoints\n    ax.text(msg, with_checkpoints[idx] - 1500, f'{int(with_checkpoints[idx]):,}',\n            ha='center', va='top', fontsize=9, color='darkgreen', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nüìä Chart Analysis:\")\nprint(\"   ‚Ä¢ RED line: Traditional approach - tokens keep growing ‚ö†Ô∏è\")\nprint(\"   ‚Ä¢ GREEN line: Checkpoints flatten growth after 50 messages ‚úÖ\")\nprint(\"   ‚Ä¢ YELLOW area: Your actual savings (grows with conversation length)\")\nprint()\nprint(\"üí° The Longer the Conversation, the Bigger Your Savings!\")\nprint(f\"   ‚Ä¢ At 150 messages: Save {without_checkpoints[5] - with_checkpoints[5]:,.0f} tokens (63%)\")\nprint(f\"   ‚Ä¢ At 500 messages: Save {savings_500:,.0f} tokens ({((savings_500/without_checkpoints[-1])*100):.0f}%)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üí∞ Part 6: Token Savings Calculation\n",
    "\n",
    "Let's calculate the actual savings from using checkpoints!"
   ],
   "metadata": {
    "id": "part5"
   }
  },
  {
   "cell_type": "code",
   "source": "## üí∞ Part 7: Token Savings & Cost Analysis\n\nLet's calculate the actual savings from using checkpoints!",
   "metadata": {
    "id": "cost-analysis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üìä Part 7: Visual Comparison Chart"
   ],
   "metadata": {
    "id": "part6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üèÅ Summary & Key Takeaways"
   ],
   "metadata": {
    "id": "summary"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"‚ïê\" * 70)\n",
    "print(\"üèÜ CHATROUTES: KEY FEATURES & BENEFITS\")\n",
    "print(\"‚ïê\" * 70)\n",
    "print()\n",
    "\n",
    "print(\"üí∞ COST SAVINGS:\")\n",
    "print(\"   ‚úì 60-70% token reduction for long conversations\")\n",
    "print(\"   ‚úì $17K+ annual savings (10K conversations/month)\")\n",
    "print(\"   ‚úì ROI of 342% in first year\")\n",
    "print(\"   ‚úì Savings scale linearly with usage\\n\")\n",
    "\n",
    "print(\"‚ö° PERFORMANCE:\")\n",
    "print(\"   ‚úì 2-3x faster responses for long conversations\")\n",
    "print(\"   ‚úì <5ms context assembly (10x better than target)\")\n",
    "print(\"   ‚úì Consistent performance regardless of conversation length\")\n",
    "print(\"   ‚úì Real-time streaming support\\n\")\n",
    "\n",
    "print(\"üîê SECURITY & COMPLIANCE:\")\n",
    "print(\"   ‚úì 100% immutable messages (database-enforced)\")\n",
    "print(\"   ‚úì SHA-256 cryptographic hashing\")\n",
    "print(\"   ‚úì Complete audit trails\")\n",
    "print(\"   ‚úì HIPAA, GDPR, SOC2 compliant\\n\")\n",
    "\n",
    "print(\"üå≥ ADVANCED FEATURES:\")\n",
    "print(\"   ‚úì Conversation branching for exploring alternatives\")\n",
    "print(\"   ‚úì AI-powered checkpointing for cost optimization\")\n",
    "print(\"   ‚úì Multi-model support (GPT-5, Claude, GPT-4, etc.)\")\n",
    "print(\"   ‚úì Intelligent context assembly\\n\")\n",
    "\n",
    "print(\"‚ïê\" * 70)\n",
    "print()\n",
    "print(\"üìö Resources:\")\n",
    "print(\"   ‚Ä¢ Documentation: https://docs.chatroutes.com\")\n",
    "print(\"   ‚Ä¢ API Reference: https://docs.chatroutes.com/api\")\n",
    "print(\"   ‚Ä¢ Python SDK: https://github.com/chatroutes/chatroutes-python-sdk\")\n",
    "print(\"   ‚Ä¢ JavaScript SDK: https://github.com/chatroutes/chatroutes-sdk\")\n",
    "print()\n",
    "print(\"üöÄ Ready to get started? Sign up at https://chatroutes.com\")\n",
    "print()\n",
    "print(\"‚ïê\" * 70)"
   ],
   "metadata": {
    "id": "final-summary"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üßπ Cleanup (Optional)"
   ],
   "metadata": {
    "id": "cleanup"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Cleaning up test conversations...\\n\")\n",
    "\n",
    "try:\n",
    "    client.conversations.delete(conv_id)\n",
    "    print(f\"‚úì Deleted conversation: {conv_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Note: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    client.conversations.delete(long_conv_id)\n",
    "    print(f\"‚úì Deleted conversation: {long_conv_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Note: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Cleanup complete!\")"
   ],
   "metadata": {
    "id": "cleanup-code"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}